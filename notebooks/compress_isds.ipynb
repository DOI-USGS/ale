{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a6615fc6-3c45-4a01-9105-762b4b27df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale\n",
    "import base64\n",
    "import brotli\n",
    "import h5py\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from functools import reduce\n",
    "from sozipfile import sozipfile as szip\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "0aeca2c7-b973-4149-a7d8-6441d33aa8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['./lroc_isd0.json','./lroc_isd1.json','./lroc_isd2.json',\n",
    "             './lroc_isd3.json','./lroc_isd4.json','./lroc_isd5.json',\n",
    "             './lroc_isd6.json','./lroc_isd7.json','./lroc_isd8.json',\n",
    "             './lroc_isd9.json']\n",
    "isds = []\n",
    "for fn in filenames:\n",
    "    with open(fn, 'r') as f:  \n",
    "        new_isd = json.load(f)\n",
    "        isds.append(new_isd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0ee5e29f-6b5e-47bb-b0ff-2f7a4a90e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "isds_altered = isds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "30e9317f-8622-435d-aa04-6384c63f30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, altered in enumerate(isds_altered):\n",
    "    diff = (1 + (i/100000000))\n",
    "    altered['instrument_pointing']['ephemeris_times'] = [ diff * num for num in altered['instrument_pointing']['ephemeris_times']]\n",
    "    altered['instrument_pointing']['quaternions'] = [[diff * num for num in quats] for quats in altered['instrument_pointing']['quaternions']]\n",
    "    altered['instrument_pointing']['angular_velocities'] = [[diff * num for num in quats] for quats in altered['instrument_pointing']['angular_velocities']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "00b302f3-83ea-4879-ac47-49a0aa370e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_multispectral_isd(outfile, json_str_list, alg='hdf5'):\n",
    "    if alg==\"hdf5\":\n",
    "        with h5py.File(outfile, 'w') as f:\n",
    "            def store_recursive(key, val, group):\n",
    "                chain = (group.name +'/'+ key).split('/')\n",
    "                # Drop empty elements from chain.  Should find a better way to to do this.\n",
    "                chain = [item for item in chain if item]\n",
    "                if isinstance(val, dict):\n",
    "                    # Create nested group in hdf5, i.e. f['group/subgroup']\n",
    "                    subgroup = group.create_group(key)\n",
    "                    for k, v in val.items():\n",
    "                        store_recursive(k,v, subgroup)\n",
    "                elif isinstance(val, str):\n",
    "                    data = np.array([val], dtype='S')\n",
    "                    try:\n",
    "                        # Use the list of keys to access same nested location in all the dictionaries\n",
    "                        data = [reduce(lambda d, k: d[k], chain, item) for item in json_str_list]\n",
    "                    except KeyError:\n",
    "                        # If key isn't found, replicate a placeholder string for all isds.\n",
    "                        #  Should probably use data where it's available instead of replacing all values with empty string\n",
    "                        data = [str(\"empty\")] * len(json_str_list)\n",
    "                    # Encode all data to strings\n",
    "                    data = list(map(lambda val: np.array([val], dtype='S'), data))\n",
    "                    group.create_dataset(key, data=data)\n",
    "                else:\n",
    "                    data = np.array([val])\n",
    "                    if data.size > 1:\n",
    "                        data = [reduce(lambda d, k: d.get(k, {'empty': []}), chain, item) for item in json_str_list]\n",
    "                        group.create_dataset(key, data=data, compression='gzip', compression_opts=9)\n",
    "                    else:\n",
    "                        if '/' in key:\n",
    "                            return\n",
    "                        data = [reduce(lambda d, k: d.get(k, {'empty': []}), chain, item) for item in json_str_list]\n",
    "                        group.create_dataset(key, data=data)\n",
    "            for k,v in json_str_list[0].items():\n",
    "                store_recursive(k,v, f)\n",
    "    elif alg==\"brotli\":\n",
    "        json_bytes = np.frombuffer(json_str.encode('utf-8'), dtype='uint8')\n",
    "        with open(outfile, 'wb') as f:\n",
    "            f.write(brotli.compress(json_bytes))\n",
    "            return True\n",
    "    elif alg==\"sozip\":\n",
    "        with szip.ZipFile(outfile, 'w', compression=szip.ZIP_DEFLATED) as f:\n",
    "            f.write(json_str, os.path.basename(outfile))\n",
    "            return True\n",
    "    elif alg==\"gdal\":\n",
    "        with gdal.Open('out.tif', gdal.GA_Update) as df:\n",
    "            def store_recursive(key, val, keyword):\n",
    "                chain = (keyword +'/'+ key).split('/')\n",
    "                # Drop empty elements from chain.  Should find a better way to to do this.\n",
    "                chain = [item for item in chain if item]\n",
    "                if isinstance(val, dict):\n",
    "                    # Create nested group in hdf5, i.e. f['group/subgroup']\n",
    "                    subgroup = keyword + '/' + key\n",
    "                    for k, v in val.items():\n",
    "                        store_recursive(k,v, subgroup)\n",
    "                elif isinstance(val, str):\n",
    "                    data = np.array([val], dtype='S')\n",
    "                    try:\n",
    "                        # Use the list of keys to access same nested location in all the dictionaries\n",
    "                        data = [reduce(lambda d, k: d[k], chain, item) for item in json_str_list]\n",
    "                    except KeyError:\n",
    "                        # If key isn't found, replicate a placeholder string for all isds.\n",
    "                        #  Should probably use data where it's available instead of replacing all values with empty string\n",
    "                        data = [str(\"empty\")] * len(json_str_list)\n",
    "                    # Encode all data to strings\n",
    "                    data = json.dumps(data)\n",
    "                    df.SetMetadataItem('/'.join(chain), data)\n",
    "                else:\n",
    "                    data = np.array([val])\n",
    "                    if data.size > 10:\n",
    "                        data = [reduce(lambda d, k: d.get(k, {'empty': []}), chain, item) for item in json_str_list]\n",
    "                        # Uncomment these lines to compress \n",
    "                        # list of lists -> list of ndarrays\n",
    "                        #data = list(map(np.array, data))\n",
    "                        # list of ndarrays -> list of bytes\n",
    "                        #data = list(map(np.ndarray.tobytes, data))\n",
    "                        # encode strings as utf-8\n",
    "                        #data = [base64.b64encode(byte_data).decode('utf-8') for byte_data in data]\n",
    "                        # dump to json \n",
    "                        data = json.dumps(data)\n",
    "                        df.SetMetadataItem('/'.join(chain), data)\n",
    "                    else:\n",
    "                        if '/' in key:\n",
    "                            return\n",
    "                        data = [reduce(lambda d, k: d.get(k, {'empty': []}), chain, item) for item in json_str_list]\n",
    "                        #group.create_dataset(key, data=data)\n",
    "                        try:\n",
    "                            data = json.dumps(data.astype(list))\n",
    "                        except:\n",
    "                            data = json.dumps(data)\n",
    "                        df.SetMetadataItem('/'.join(chain), data)\n",
    "            for k,v in json_str_list[0].items():\n",
    "                store_recursive(k,v, '')\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2b9a922f-4c1b-4ba8-93bd-6fb23aa3bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf5(hdf5_file):\n",
    "    def read_recursive(group):\n",
    "        result = {}\n",
    "        for key, item in group.items():\n",
    "            if isinstance(item, h5py.Group): \n",
    "                result[key] = read_recursive(item)\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                data = item[:]\n",
    "                decoded = decode_data(data)\n",
    "                if (key == \"ephemeris_times\" or key == \"constant_frames\") and not hasattr(decoded, '__iter__'):\n",
    "                    result[key] = [decoded]\n",
    "                else:\n",
    "                    result[key] = decoded\n",
    "                \n",
    "        return result\n",
    "    def decode_data(data):\n",
    "        \"\"\"Convert NumPy arrays to raw Python data types.\"\"\"\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.char == 'S' or data.dtype.char == 'U': \n",
    "                return data.astype(str)[0]\n",
    "            elif np.issubdtype(data.dtype, np.number):\n",
    "                if data.size == 1:\n",
    "                    return data.item()\n",
    "                else:\n",
    "                    return data[0].tolist()\n",
    "            else:\n",
    "                return data.tolist()\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as f:\n",
    "        # Start the recursive reading from the root of the file\n",
    "        data = read_recursive(f)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "9512b86e-a251-4f27-a868-c108804e04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_brotli(compressed_json_file):\n",
    "    with open(compressed_json_file, 'rb') as f:\n",
    "        data = brotli.decompress(f.read())\n",
    "    isd = json.loads(data)\n",
    "    return isd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "ce2b798c-ce71-4dd2-9e6a-a663b91851df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sozip(zip_file):\n",
    "    with szip.ZipFile(zip_file, 'r') as f:\n",
    "        data = f.read(\"lroc.zip\")\n",
    "    isd = json.loads(data.decode('utf-8'))\n",
    "    return isd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "23746aad-8306-4eca-b369-3daaec29fc44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test that compression/decompression yields identical isds\n",
    "#%timeit compress_isd(\"lroc.h5\", original_isd, \"hdf5\")\n",
    "#compress_multispectral_isd(\"lroc_transpose.h5\", isds, \"hdf5\")\n",
    "\n",
    "compress_multispectral_isd(\"out.tif\", isds_altered, \"gdal\")\n",
    "\n",
    "\n",
    "#print(\"compress brotli: \")\n",
    "#%timeit compress_isd(\"lroc.br\", json.dumps(original_isd), \"brotli\")\n",
    "#print(\"compress sozip: \")\n",
    "#%timeit compress_isd(\"lroc.zip\", filename, \"sozip\")\n",
    "#%timeit hdf5_isd = read_hdf5(\"lroc.h5\")\n",
    "\n",
    "#print(\"decompress brotli: \")\n",
    "#%timeit brotli_isd = read_brotli(\"lroc.br\")\n",
    "\n",
    "#print(\"decompress sozip: \")\n",
    "#%timeit sozip_isd = read_sozip(\"lroc.zip\")\n",
    "\n",
    "\n",
    "#print(f\"HDF5: {hdf5_isd == original_isd}\")\n",
    "#print(f\"brotli: {brotli_isd == original_isd}\")\n",
    "#print(f\"sozip: {sozip_isd == original_isd}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "1ba5af5d-8d85-475a-84fb-d9f33f00b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compress_multispectral_isd(\"lroc_altered_transpose.h5\", isds_altered, \"hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0f27ad0c-357d-45f6-879e-0f998923e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_isd = read_hdf5('lroc_transpose.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "9f2bf3a4-8b4b-4fda-8900-b3158cf28bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['body_rotation', 'center_ephemeris_time', 'detector_center', 'detector_line_summing', 'detector_sample_summing', 'focal2pixel_lines', 'focal2pixel_samples', 'focal_length_model', 'image_lines', 'image_samples', 'instrument_pointing', 'instrument_position', 'interpolation_method', 'isis_camera_version', 'line_scan_rate', 'naif_keywords', 'name_model', 'name_platform', 'name_sensor', 'optical_distortion', 'radii', 'reference_height', 'starting_detector_line', 'starting_detector_sample', 'starting_ephemeris_time', 'sun_position']>\n",
      "[[[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]\n",
      "\n",
      " [[ 6.22675585e-08 -1.02491422e-06  2.45531633e-06]\n",
      "  [ 6.22675607e-08 -1.02491422e-06  2.45531633e-06]]]\n",
      "<HDF5 dataset \"isis_camera_version\": shape (10,), type \"<i8\">\n",
      "[[[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]\n",
      "\n",
      " [[ 5.00000000e-01 -2.58657795e+01  9.90570599e-04]]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('lroc_transpose.h5', 'r') as f:\n",
    "    # Start the recursive reading from the root of the file\n",
    "    print(f.keys())\n",
    "    print(f['body_rotation']['angular_velocities'][:])\n",
    "    print(f['isis_camera_version'])\n",
    "    print(f['line_scan_rate'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d6b48-4087-4238-b68f-bf605c7bdcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
